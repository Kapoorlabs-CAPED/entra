{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 3D Covariance Optimization with Divergence-Free Transformation\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Sampling points from a 3D uniform distribution\n",
    "2. Creating a divergence-free transformation with multiple centers\n",
    "3. Optimizing coefficients to minimize covariance determinant\n",
    "4. Visualizing 2D projections (XY, XZ, YZ) before and after\n",
    "5. If successful, each 2D projection should appear Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from entra import (\n",
    "    VectorSampler, TensorBasis, Transformation, CovarianceMinimizer,\n",
    "    EffectiveTransformation, EffectiveCovarianceMinimizer,\n",
    "    shannon_entropy_gaussian, shannon_entropy_uniform, shannon_entropy_knn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_covariance_ellipse(ax, mean, cov, n_std=2, **kwargs):\n",
    "    \"\"\"Plot covariance ellipse for 2D.\"\"\"\n",
    "    from matplotlib.patches import Ellipse\n",
    "    \n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    order = eigvals.argsort()[::-1]\n",
    "    eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n",
    "    \n",
    "    angle = np.degrees(np.arctan2(*eigvecs[:, 0][::-1]))\n",
    "    width, height = 2 * n_std * np.sqrt(eigvals)\n",
    "    \n",
    "    ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, **kwargs)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "def plot_2d_projections(points, title_prefix, entropy=None, fig=None, axes=None):\n",
    "    \"\"\"Plot XY, XZ, YZ projections of 3D points.\"\"\"\n",
    "    if fig is None or axes is None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    projections = [\n",
    "        (0, 1, 'XY'),\n",
    "        (0, 2, 'XZ'),\n",
    "        (1, 2, 'YZ')\n",
    "    ]\n",
    "    \n",
    "    for ax, (i, j, name) in zip(axes, projections):\n",
    "        pts_2d = points[:, [i, j]]\n",
    "        mean_2d = np.mean(pts_2d, axis=0)\n",
    "        cov_2d = np.cov(pts_2d, rowvar=False)\n",
    "        det_2d = np.linalg.det(cov_2d)\n",
    "        \n",
    "        ax.scatter(pts_2d[:, 0], pts_2d[:, 1], alpha=0.3, s=10)\n",
    "        plot_covariance_ellipse(ax, mean_2d, cov_2d, n_std=2,\n",
    "                                fill=False, color='red', linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel(['X', 'X', 'Y'][projections.index((i, j, name))])\n",
    "        ax.set_ylabel(['Y', 'Z', 'Z'][projections.index((i, j, name))])\n",
    "        ax.set_title(f'{name} Projection\\ndet = {det_2d:.2e}')\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if entropy is not None:\n",
    "        fig.suptitle(f'{title_prefix} (H = {entropy:.4f} nats)', fontsize=14)\n",
    "    else:\n",
    "        fig.suptitle(title_prefix, fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Sample Points from 3D Uniform Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "D = 3\n",
    "num_points_per_dim = 10  # 10^3 = 1000 points\n",
    "delta_x = 1\n",
    "sigma = 3\n",
    "\n",
    "# Create sampler centered at origin\n",
    "sampler = VectorSampler(\n",
    "    center=[0.0, 0.0, 0.0],\n",
    "    delta_x=delta_x,\n",
    "    num_points_per_dim=num_points_per_dim,\n",
    "    distribution=\"uniform\"\n",
    ")\n",
    "\n",
    "# Sample the grid points\n",
    "eval_points = sampler.sample()\n",
    "J = sampler.J\n",
    "grid_shape = sampler.num_points_per_dim\n",
    "\n",
    "print(f\"Grid: {num_points_per_dim}^{D} = {J} points\")\n",
    "print(f\"Grid shape: {grid_shape}\")\n",
    "print(f\"delta_x = {delta_x}\")\n",
    "print(f\"sigma = {sigma}\")\n",
    "print(f\"\\neval_points shape: {eval_points.shape}  (J, D)\")\n",
    "print(f\"\\nGrid extent:\")\n",
    "print(f\"  x: [{eval_points[:, 0].min():.2f}, {eval_points[:, 0].max():.2f}]\")\n",
    "print(f\"  y: [{eval_points[:, 1].min():.2f}, {eval_points[:, 1].max():.2f}]\")\n",
    "print(f\"  z: [{eval_points[:, 2].min():.2f}, {eval_points[:, 2].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial statistics\n",
    "initial_mean = np.mean(eval_points, axis=0)\n",
    "initial_cov = np.cov(eval_points, rowvar=False)\n",
    "initial_det = np.linalg.det(initial_cov)\n",
    "initial_entropy_uniform = shannon_entropy_uniform(eval_points)\n",
    "initial_entropy_knn = shannon_entropy_knn(eval_points)\n",
    "\n",
    "print(\"Initial Distribution Statistics (Uniform):\")\n",
    "print(f\"  Mean: [{initial_mean[0]:.4f}, {initial_mean[1]:.4f}, {initial_mean[2]:.4f}]\")\n",
    "print(f\"  Covariance diagonal: [{initial_cov[0,0]:.4f}, {initial_cov[1,1]:.4f}, {initial_cov[2,2]:.4f}]\")\n",
    "print(f\"  Determinant: {initial_det:.6e}\")\n",
    "print(f\"  Volume: {num_points_per_dim * delta_x}^{D} = {(num_points_per_dim * delta_x)**D}\")\n",
    "print(f\"  Entropy (uniform): {initial_entropy_uniform:.6f} nats\")\n",
    "print(f\"  Entropy (k-NN):    {initial_entropy_knn:.6f} nats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize initial 2D projections\n",
    "fig, axes = plot_2d_projections(eval_points, 'INITIAL: Uniform Distribution', initial_entropy_uniform)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 2: Create Centers and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centers along each axis\n",
    "center_list = []\n",
    "max_coord = int(eval_points[:, 0].max())\n",
    "for i in range(max_coord):\n",
    "    # Along x-axis\n",
    "    center_list.append([i, 0.0, 0.0])\n",
    "    center_list.append([-i, 0.0, 0.0])\n",
    "    # Along y-axis\n",
    "    center_list.append([0.0, i, 0.0])\n",
    "    center_list.append([0.0, -i, 0.0])\n",
    "    # Along z-axis\n",
    "    center_list.append([0.0, 0.0, i])\n",
    "    center_list.append([0.0, 0.0, -i])\n",
    "\n",
    "centers = np.asarray(center_list)\n",
    "print(f\"Number of centers: {len(centers)}\")\n",
    "print(f\"Centers shape: {centers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basis and transformation\n",
    "basis = TensorBasis(centers, sigma=sigma)\n",
    "transformation = Transformation(basis)\n",
    "\n",
    "print(f\"TensorBasis: L={basis.L}, D={basis.D}, sigma={sigma}\")\n",
    "print(f\"Transformation: {transformation.num_parameters} parameters (L x D = {basis.L} x {basis.D})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 3: Optimize Coefficients (Levenberg-Marquardt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimizer\n",
    "minimizer = CovarianceMinimizer(transformation, eval_points)\n",
    "\n",
    "# Track optimization history\n",
    "history = {\n",
    "    'iteration': [],\n",
    "    'determinant': [],\n",
    "    'entropy': [],\n",
    "    'log_det': []\n",
    "}\n",
    "\n",
    "# Custom optimization to record each step\n",
    "x = transformation.get_coefficients_flat().copy()\n",
    "n_params = len(x)\n",
    "\n",
    "lam = 1.0  # Damping\n",
    "eps = 1e-7\n",
    "max_iter = 500\n",
    "tol = 1e-12\n",
    "\n",
    "# Record initial\n",
    "cov = minimizer.compute_covariance(x)\n",
    "det_val = np.linalg.det(cov)\n",
    "entropy = shannon_entropy_gaussian(cov)\n",
    "history['iteration'].append(0)\n",
    "history['determinant'].append(det_val)\n",
    "history['entropy'].append(entropy)\n",
    "history['log_det'].append(np.log(det_val))\n",
    "\n",
    "print(f\"{'Iter':>5}  {'Determinant':>14}  {'Entropy':>12}  {'log(det)':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{0:>5}  {det_val:>14.6e}  {entropy:>12.6f}  {np.log(det_val):>12.6f}\")\n",
    "\n",
    "for iteration in range(1, max_iter + 1):\n",
    "    r = minimizer.residuals_for_lm(x)\n",
    "    \n",
    "    # Jacobian\n",
    "    J_mat = np.zeros((len(r), n_params))\n",
    "    for i in range(n_params):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += eps\n",
    "        J_mat[:, i] = (minimizer.residuals_for_lm(x_plus) - r) / eps\n",
    "    \n",
    "    # LM step\n",
    "    JTJ = J_mat.T @ J_mat\n",
    "    JTr = J_mat.T @ r\n",
    "    \n",
    "    try:\n",
    "        delta = np.linalg.solve(JTJ + lam * np.eye(n_params), -JTr)\n",
    "    except:\n",
    "        delta = -JTr / (np.diag(JTJ) + lam + 1e-10)\n",
    "    \n",
    "    x_new = x + delta\n",
    "    obj_new = minimizer.objective_logdet(x_new)\n",
    "    obj_old = minimizer.objective_logdet(x)\n",
    "    \n",
    "    if obj_new < obj_old:\n",
    "        x = x_new\n",
    "        lam *= 0.1\n",
    "        improvement = abs(obj_old - obj_new)\n",
    "        \n",
    "        cov = minimizer.compute_covariance(x)\n",
    "        det_val = np.linalg.det(cov)\n",
    "        entropy = shannon_entropy_gaussian(cov)\n",
    "        \n",
    "        history['iteration'].append(iteration)\n",
    "        history['determinant'].append(det_val)\n",
    "        history['entropy'].append(entropy)\n",
    "        history['log_det'].append(np.log(det_val))\n",
    "        \n",
    "        if iteration % 20 == 0 or iteration <= 5:\n",
    "            print(f\"{iteration:>5}  {det_val:>14.6e}  {entropy:>12.6f}  {np.log(det_val):>12.6f}\")\n",
    "        \n",
    "        if improvement < tol:\n",
    "            print(f\"\\nConverged at iteration {iteration}\")\n",
    "            break\n",
    "    else:\n",
    "        lam *= 10.0\n",
    "\n",
    "print(f\"\\nFinal: det = {det_val:.6e}, entropy = {entropy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set final coefficients\n",
    "transformation.set_coefficients_flat(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 4: Optimization Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Determinant\n",
    "ax0 = axes[0]\n",
    "ax0.semilogy(history['iteration'], history['determinant'], 'b.-', linewidth=2, markersize=4)\n",
    "ax0.set_xlabel('Iteration', fontsize=12)\n",
    "ax0.set_ylabel('det(Cov)', fontsize=12)\n",
    "ax0.set_title('Determinant vs Iteration', fontsize=14)\n",
    "ax0.grid(True, alpha=0.3)\n",
    "\n",
    "# Log determinant\n",
    "ax1 = axes[1]\n",
    "ax1.plot(history['iteration'], history['log_det'], 'g.-', linewidth=2, markersize=4)\n",
    "ax1.set_xlabel('Iteration', fontsize=12)\n",
    "ax1.set_ylabel('log(det(Cov))', fontsize=12)\n",
    "ax1.set_title('Log-Determinant vs Iteration', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy\n",
    "ax2 = axes[2]\n",
    "ax2.plot(history['iteration'], history['entropy'], 'r.-', linewidth=2, markersize=4)\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Entropy (nats)', fontsize=12)\n",
    "ax2.set_title('Shannon Entropy vs Iteration', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 5: Transform Points and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform points\n",
    "points_transformed = transformation.transform(eval_points)\n",
    "\n",
    "# Final statistics\n",
    "final_mean = np.mean(points_transformed, axis=0)\n",
    "final_cov = np.cov(points_transformed, rowvar=False)\n",
    "final_det = np.linalg.det(final_cov)\n",
    "final_entropy_gaussian = shannon_entropy_gaussian(final_cov)\n",
    "final_entropy_knn = shannon_entropy_knn(points_transformed)\n",
    "\n",
    "print(\"Comparison:\")\n",
    "print(f\"{'':20} {'Initial':>14} {'Final':>14} {'Change':>14}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Determinant':20} {initial_det:>14.6e} {final_det:>14.6e} {initial_det/final_det:>14.2f}x\")\n",
    "print(f\"{'Entropy (uniform)':20} {initial_entropy_uniform:>14.6f}\")\n",
    "print(f\"{'Entropy (gaussian)':20} {'':>14} {final_entropy_gaussian:>14.6f}\")\n",
    "print(f\"{'Entropy (k-NN)':20} {initial_entropy_knn:>14.6f} {final_entropy_knn:>14.6f} {initial_entropy_knn - final_entropy_knn:>14.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 6: Outer Loop with Updated Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get updated basis after first iteration\n",
    "updated_basis = transformation.get_updated_basis(eval_points)  # (J, L, D)\n",
    "\n",
    "# Outer loop with updated basis\n",
    "N_OUTER = 5\n",
    "outer_history = {\n",
    "    'round': [0], \n",
    "    'determinant': [final_det], \n",
    "    'entropy': [final_entropy_gaussian],\n",
    "    'points': [points_transformed.copy()],\n",
    "    'covariance': [final_cov.copy()]\n",
    "}\n",
    "current_basis = updated_basis.copy()\n",
    "current_points = points_transformed.copy()\n",
    "\n",
    "print(f\"Starting outer loop with {N_OUTER} rounds\")\n",
    "print(f\"Initial basis shape: {current_basis.shape}\")\n",
    "print(f\"\\n{'Round':>5}  {'Determinant':>14}  {'Entropy':>12}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{0:>5}  {final_det:>14.6e}  {final_entropy_gaussian:>12.6f}\")\n",
    "\n",
    "for outer_round in range(1, N_OUTER + 1):\n",
    "    # Create effective transformation with current basis\n",
    "    eff_transform = EffectiveTransformation(current_basis, current_points)\n",
    "    eff_minimizer = EffectiveCovarianceMinimizer(eff_transform)\n",
    "    \n",
    "    # Optimize\n",
    "    result = eff_minimizer.optimize(max_iterations=500, tolerance=1e-10, verbose=False)\n",
    "    \n",
    "    # Record results\n",
    "    round_det = result['final_determinant']\n",
    "    round_cov = result['final_covariance']\n",
    "    round_entropy = shannon_entropy_gaussian(round_cov)\n",
    "    \n",
    "    # Update points for next round\n",
    "    current_points = eff_transform.transform()\n",
    "    current_basis = eff_transform.get_updated_basis()\n",
    "    \n",
    "    outer_history['round'].append(outer_round)\n",
    "    outer_history['determinant'].append(round_det)\n",
    "    outer_history['entropy'].append(round_entropy)\n",
    "    outer_history['points'].append(current_points.copy())\n",
    "    outer_history['covariance'].append(round_cov.copy())\n",
    "    \n",
    "    print(f\"{outer_round:>5}  {round_det:>14.6e}  {round_entropy:>12.6f}\")\n",
    "\n",
    "print(f\"\\nFinal reduction: {outer_history['determinant'][0] / outer_history['determinant'][-1]:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outer loop progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax0 = axes[0]\n",
    "ax0.semilogy(outer_history['round'], outer_history['determinant'], 'bo-', linewidth=2, markersize=10)\n",
    "ax0.axhline(initial_det, color='r', linestyle='--', label=f'Initial: {initial_det:.2e}')\n",
    "ax0.set_xlabel('Outer Round', fontsize=12)\n",
    "ax0.set_ylabel('det(Cov)', fontsize=12)\n",
    "ax0.set_title('Determinant vs Outer Round', fontsize=14)\n",
    "ax0.legend()\n",
    "ax0.grid(True, alpha=0.3)\n",
    "\n",
    "ax1 = axes[1]\n",
    "ax1.plot(outer_history['round'], outer_history['entropy'], 'go-', linewidth=2, markersize=10)\n",
    "ax1.axhline(initial_entropy_uniform, color='r', linestyle='--', label=f'Initial (uniform): {initial_entropy_uniform:.4f}')\n",
    "ax1.set_xlabel('Outer Round', fontsize=12)\n",
    "ax1.set_ylabel('Entropy (nats)', fontsize=12)\n",
    "ax1.set_title('Entropy vs Outer Round', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Step 7: Compare 2D Projections Before and After\n",
    "\n",
    "If the optimization worked, each 2D projection should look Gaussian (elliptical contours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final transformed points\n",
    "final_points = outer_history['points'][-1]\n",
    "final_cov_outer = outer_history['covariance'][-1]\n",
    "final_entropy_outer = outer_history['entropy'][-1]\n",
    "\n",
    "# Before: Initial uniform distribution\n",
    "print(\"BEFORE: Initial Uniform Distribution\")\n",
    "fig1, axes1 = plot_2d_projections(eval_points, 'BEFORE: Uniform Distribution', initial_entropy_uniform)\n",
    "plt.show()\n",
    "\n",
    "# After: Final transformed distribution\n",
    "print(\"\\nAFTER: Final Transformed Distribution\")\n",
    "fig2, axes2 = plot_2d_projections(final_points, 'AFTER: Transformed Distribution', final_entropy_outer)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison for each projection\n",
    "projections = [\n",
    "    (0, 1, 'XY', 'X', 'Y'),\n",
    "    (0, 2, 'XZ', 'X', 'Z'),\n",
    "    (1, 2, 'YZ', 'Y', 'Z')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 15))\n",
    "\n",
    "for row, (i, j, name, xlabel, ylabel) in enumerate(projections):\n",
    "    # Before\n",
    "    ax_before = axes[row, 0]\n",
    "    pts_before = eval_points[:, [i, j]]\n",
    "    mean_before = np.mean(pts_before, axis=0)\n",
    "    cov_before = np.cov(pts_before, rowvar=False)\n",
    "    det_before = np.linalg.det(cov_before)\n",
    "    \n",
    "    ax_before.scatter(pts_before[:, 0], pts_before[:, 1], alpha=0.3, s=10, c='blue')\n",
    "    plot_covariance_ellipse(ax_before, mean_before, cov_before, n_std=2,\n",
    "                            fill=False, color='red', linewidth=2)\n",
    "    ax_before.set_xlabel(xlabel)\n",
    "    ax_before.set_ylabel(ylabel)\n",
    "    ax_before.set_title(f'BEFORE: {name} Projection\\ndet = {det_before:.2e}')\n",
    "    ax_before.set_aspect('equal')\n",
    "    ax_before.grid(True, alpha=0.3)\n",
    "    \n",
    "    # After\n",
    "    ax_after = axes[row, 1]\n",
    "    pts_after = final_points[:, [i, j]]\n",
    "    mean_after = np.mean(pts_after, axis=0)\n",
    "    cov_after = np.cov(pts_after, rowvar=False)\n",
    "    det_after = np.linalg.det(cov_after)\n",
    "    \n",
    "    ax_after.scatter(pts_after[:, 0], pts_after[:, 1], alpha=0.3, s=10, c='orange')\n",
    "    plot_covariance_ellipse(ax_after, mean_after, cov_after, n_std=2,\n",
    "                            fill=False, color='red', linewidth=2)\n",
    "    ax_after.set_xlabel(xlabel)\n",
    "    ax_after.set_ylabel(ylabel)\n",
    "    ax_after.set_title(f'AFTER: {name} Projection\\ndet = {det_after:.2e}')\n",
    "    ax_after.set_aspect('equal')\n",
    "    ax_after.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('2D Projections: Before vs After Transformation', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 8: Distribution Evolution Through Outer Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show evolution of XY projection through outer loop\n",
    "n_plots = 1 + len(outer_history['round'])  # Initial + all rounds\n",
    "n_cols = min(4, n_plots)\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "axes = np.atleast_2d(axes).flatten()\n",
    "\n",
    "# Plot 0: Initial uniform (XY projection)\n",
    "ax = axes[0]\n",
    "pts_2d = eval_points[:, [0, 1]]\n",
    "mean_2d = np.mean(pts_2d, axis=0)\n",
    "cov_2d = np.cov(pts_2d, rowvar=False)\n",
    "ax.scatter(pts_2d[:, 0], pts_2d[:, 1], alpha=0.3, s=10, c='blue')\n",
    "plot_covariance_ellipse(ax, mean_2d, cov_2d, n_std=2, fill=False, color='red', linewidth=2)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title(f'Initial (Uniform)\\nH = {initial_entropy_uniform:.4f}')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 1+: After each outer round\n",
    "for idx, (rnd, pts, ent) in enumerate(zip(\n",
    "    outer_history['round'], \n",
    "    outer_history['points'],\n",
    "    outer_history['entropy']\n",
    ")):\n",
    "    ax = axes[idx + 1]\n",
    "    pts_2d = pts[:, [0, 1]]\n",
    "    mean_2d = np.mean(pts_2d, axis=0)\n",
    "    cov_2d = np.cov(pts_2d, rowvar=False)\n",
    "    \n",
    "    color = plt.cm.viridis(idx / max(1, len(outer_history['round']) - 1))\n",
    "    ax.scatter(pts_2d[:, 0], pts_2d[:, 1], alpha=0.3, s=10, c=[color])\n",
    "    plot_covariance_ellipse(ax, mean_2d, cov_2d, n_std=2, fill=False, color='red', linewidth=2)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    if rnd == 0:\n",
    "        ax.set_title(f'After 1st Iter\\nH = {ent:.4f}')\n",
    "    else:\n",
    "        ax.set_title(f'Outer Round {rnd}\\nH = {ent:.4f}')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(n_plots, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('XY Projection Evolution Through Optimization', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The 3D optimization:\n",
    "- Started with a uniform distribution on a 3D grid\n",
    "- Applied divergence-free transformations to minimize covariance determinant\n",
    "- Each 2D projection should show a transition from uniform (rectangular) to Gaussian (elliptical)\n",
    "- The covariance ellipses in each projection indicate the shape of the distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
